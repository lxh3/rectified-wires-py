# TO DO
# Do more optimization for speed
# add nopython if possible to numba in sda.py
# make argparse print nicely
# update gitignore before pushing first commit

import time
start_time = time.time()

import sys
import argparse
import numpy as np

from rectified_wires_net import RectifiedWiresNet
from sda import SDA

def check_compat(trainf, testf):
    """ 
    Checks compatibility of training and test files and
    determines several useful values to run the training

    Inputs:
    * trainf - trainfile to read training data from
    * testf - testfile to read testing data from

    Outputs:
    * vecsize - size of the data vectors
    * classnum - number of classes
    * symsize - whether the data is analog or symbolic (1 - analog, >1 - symbolic)
    * amax - maximum value for analog data (otherwise None)
    * pos - conversion of symbolic data into a numerical vector
    * trainstart - position of first data line in training file
    * teststart - position of first data line in testing file
    """

    errmsg = 'trainfile and testfile not compatible\n'
    pos = np.zeros(256)

    symsize = int(trainf.readline())
    symsize2 = int(testf.readline())

    if symsize!=symsize2:
        exit(errmsg)

    if symsize==1: 

        amax = int(trainf.readline())
        amax2 = int(testf.readline())

        datasize, classnum = [int(i) for i in trainf.readline().split()]
        datasize2, classnum2 = [int(i) for i in testf.readline().split()]

        if amax!=amax2 or datasize!=datasize2 or classnum!=classnum2:
            exit(errmsg)


        vecsize = 2*datasize

    else:

        amax = None
        sym = list(trainf.readline().strip())
        sym2 = list(testf.readline().strip())

        for i in xrange(symsize):
            if sym[i]!=sym2[i]:
                exit(errmsg)
            pos[ord(sym[i])] = i

        datasize, classnum = [int(i) for i in trainf.readline().split()]
        datasize2, classnum2 = [int(i) for i in testf.readline().split()]

        if datasize!=datasize2 or classnum!=classnum2:
            exit(errmsg)

        vecsize = symsize*datasize;

    trainstart = trainf.tell()
    teststart = testf.tell()

    return vecsize, classnum, symsize, amax, pos, trainstart, teststart


def print_log_header(logfile,trainfile,testfile,depth):

    """ 
    Prints the training file, testing file and column headers 
    to the logfile

    Inputs:
    * logfile - name of the logfile to write to
    * trainfile - name of the file with the training data
    * testfile - name of the file with the test data
    * depth  - depth of the network
  
    Outputs:
    * header written in logfile
    """

    with open(logfile,'w') as lf:
        lf.write('trainfile: '+trainfile+'\n')
        lf.write('testfile: '+testfile+'\n')
        lf.write('      data     false      iter aveiter   true   zero')
        for d in xrange(depth):
            lf.write('    act  '+str(d))
        lf.write('\n')
    return


def printnet(netfile,sda):
    """ 
    Writes the updated network with biases post training

    Inputs:
    * netfile - name of the file to write the updated network to
    * sda - Sequential Deactivation Algorithm class which contains network 
            data and updated bias values at the end of training
  
    Outputs:
    * updated network written to netfile
    """

    with open(netfile,'w') as nf:

        nf.write(str(sda.depth)+'\n')
        for d in xrange(sda.depth):
            nf.write(str(sda.size[d])+' ')
        nf.write('\n'+str(sda.edgenum)+'\n\n')

        for d in xrange(sda.depth):
            for k in xrange(sda.size[d]):

                innode = sda.node[d][k]

                for i in xrange(sda.outsize[innode]):

                    outnode = sda.outlist[innode][i]
                    edgeout = sda.outedge[innode][i]

                    nf.write(str(innode)+' '+str(outnode)+' '+str(sda.bias[edgeout])+'\n')

    return 

def main():
    """ 
    Runs SDA algorithm to train a sparse expander network on
    data and test its performance 

    Inputs:
    * See parser arguments below
  
    Outputs:
    * testing peformance during training written to logfile
    * updated network written to netfile
    """

    parser = argparse.ArgumentParser()
    parser.add_argument('trainfile', type=str, help='data file used for training')
    parser.add_argument('testfile', type=str, help='data file used for testing')
    parser.add_argument('netfile', type=str, help='network file, such as generated by expander')
    parser.add_argument('trainstop', type=int, help='stop training after this many data items, read cyclically from trainfile')
    parser.add_argument('trainbatch', type=int, help='number of data between each line-entry of logfile')
    parser.add_argument('testbatch', type=int, help='number of testdata items to use for testing (if less than size of testdata')

    parser.add_argument('--unf', nargs='?', type=str, help='file to which the updated network is saved')
    parser.add_argument('--lf', nargs='?',type=str, help='log file test results print to during training')
    parser.add_argument('--directory', nargs='?', type=str, help='path to training and test data')
    parser.add_argument('-v', help='increase output verbosity', action = 'store_true') 

    args = parser.parse_args()

    if args.unf is None:
        args.unf = 'updated_'+args.netfile

    if args.lf is None:
        args.lf = args.netfile+'.log'

    if args.directory is None:
        args.directory = '../data/'

    if args.v:
        print '\nPerforming SDA optimization with the following inputs:\n'
        for arg in vars(args):
            print arg, getattr(args, arg)
        
    # Open training and test files
    try:
        with open(args.directory+args.trainfile, 'r') as trainf, open(args.directory+args.testfile, 'r') as testf:
            
            # Check compatibility
            vecsize, classnum, symsize, amax, pos, trainstart, teststart = check_compat(trainf, testf)

            # Initialize Sparse Expander Net instance from file
            if args.v:
                print '\nLoading in the network.\n'
            net = RectifiedWiresNet(netfile=args.netfile, classnum=classnum, vecsize=vecsize)

            # Write header to logfile
            print_log_header(args.lf,args.trainfile,args.testfile,net.depth)

            # Initialize SDA training algo
            if args.v:
                print 'Beginning the Sequential Deactivation Algorithm\n'
            sda = SDA(net, vecsize, classnum, symsize, amax, pos, trainf, testf, trainstart, teststart, args.v)

            avemax = 0.

            for t in xrange(0,args.trainstop,args.trainbatch):
                if args.v:
                    print 'Starting training iteration '+str(t)
                aveiter = sda.train(args.trainbatch, t) # Train on trainbatch instances
             
                avetrue, avezero, aveact = sda.test(args.testbatch) # Test on testbatch instances
 
                # Write results of test to logfile
                with open(args.lf,'a') as lf: 
                    lf.write('%10.2E%10.2E%10.2E%8.2f%7.2f%7.2f'
                             %(float(t+args.trainbatch),float(sda.falsecount),float(sda.itercount),aveiter,100*avetrue,100*avezero))
                    for d in xrange(net.depth):
                        lf.write('%10.2E'%(1.-aveact[d]))
                    lf.write('\n')

                # If performance has improved, write new network to updated netfile
                if avetrue>avemax:
                    avemax = avetrue
                    printnet(args.unf,sda)  
            
                if aveiter==0. or avezero>sda.ZEROSTOP:
                    break

            # Write final results of training/testing to logfile         
            with open(args.lf,'a') as lf:
                lf.write('\nbest: '+str(100*avemax)+'\ntotal false: '+str(sda.falsecount)+'\ntotal iterations: '+str(sda.itercount))
    
    except IOError as e:
        print 'Operation failed: %s' % e.strerror
 
    return

if __name__ == '__main__':

    main() # Perform training and testing
  
    print('\nComputation took %s seconds' % (time.time() - start_time)) 
